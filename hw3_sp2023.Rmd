---
title: "Modern Data Mining, HW 3"
author:
- Group Member Christian Benitez
- Group Member Taurean Butler
- Group Member NA
date: 'Due: 11:59Pm,  2/26, 2023'
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
    latex_engine: xelatex
  html_document:
    code_folding: hide
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output
# check if you have ISLR package, if not, install it
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(ISLR, readxl, magrittr, dplyr, ggplot2,GGally, stargazer,car, leaps) # add the packages needed
```


\pagebreak

# Overview

Multiple regression is one of the most popular methods used in statistics as well as in machine learning. We use linear models as a working model for its simplicity and interpretability. It is important that we use domain knowledge as much as we could to determine the form of the response as well as the function format for the factors. Then, when we have many possible features to be included in the working model it is inevitable that we need to choose a best possible model with a sensible criterion. `Cp`, `BIC` and regularizations such as LASSO are introduced. Be aware that if a model selection is done formally or informally, the inferences obtained with the final `lm()` fit may not be valid. Some adjustment will be needed. This last step is beyond the scope of this class. Check the current research line that Linda and collaborators are working on. 

This homework consists of two parts: the first one is an exercise (you will feel it being a toy example after the covid case study) to get familiar with model selection skills such as, `Cp` and `BIC`. The main job is a rather involved case study about devastating covid19 pandemic.  Please read through the case study first.  This project is for sure a great one listed in your CV. 

For covid case study, the major time and effort would be needed in EDA portion.

## Objectives

- Model building process

- Methods
    - Model selection
        + All subsets
        + Forward/Backward
    - Regularization
        + LASSO (L1 penalty)
        + Ridge (L2 penalty)
        + Elastic net
- Understand the criteria 
    - `Cp`
    - Testing Errors
    - `BIC` 
    - `K fold Cross Validation`
    - `LASSO` 
- Packages
    - `lm()`, `Anova`
    - `regsubsets()`
    - `glmnet()` & `cv.glmnet()`

# Review materials

- Study lecture: Model selection
- Study lecture: Regularization
- Study lecture: Multiple regression

Review the code and concepts covered during lectures: multiple regression, model selection and penalized regression through elastic net. 

# Homework 2, Case study 3: Auto data set  

## Case study 3: Auto data set

This question utilizes the `Auto` dataset from ISLR. The original dataset contains 408 observations about cars. It is similar to the CARS dataset that we use in our lectures. To get the data, first install the package ISLR. The `Auto` dataset should be loaded automatically. We'll use this dataset to practice the methods learn so far. 
Original data source is here: https://archive.ics.uci.edu/ml/datasets/auto+mpg

Get familiar with this dataset first. Tip: you can use the command `?ISLR::Auto` to view a description of the dataset. 

### EDA
Explore the data, with particular focus on pairwise plots and summary statistics. Briefly summarize your findings and any peculiarities in the data.

> **Based on the EDA shown below, there does not appear to be pecularities in the data. We might consider removing the names variable for our analysis**

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} \\ 
\hline \\[-1.8ex] 
mpg & 392 & 23.400 & 7.800 & 9.000 & 46.600 \\ 
cylinders & 392 & 5.470 & 1.710 & 3 & 8 \\ 
displacement & 392 & 194.000 & 105.000 & 68.000 & 455.000 \\ 
horsepower & 392 & 104.000 & 38.500 & 46 & 230 \\ 
weight & 392 & 2,978.000 & 849.000 & 1,613 & 5,140 \\ 
acceleration & 392 & 15.500 & 2.760 & 8.000 & 24.800 \\ 
year & 392 & 76.000 & 3.680 & 70 & 82 \\ 
origin & 392 & 1.580 & 0.806 & 1 & 3 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

```{r}
head(Auto)
names(Auto)
str(Auto)
dim(Auto)
skimr::skim(Auto)

GGally::ggpairs(Auto[,-9])

```


### What effect does `time` have on `MPG`?

a) Start with a simple regression of `mpg` vs. `year` and report R's `summary` output. Is `year` a significant variable at the .05 level? State what effect `year` has on `mpg`, if any, according to this model. 

> **Comparing mpg and year, we find that year is a significant predictor on mpg according to this model with the p value being <2e-16. According to the estimate of  1.2300, we see that with each incremental increase of year mpg increases by 1.23. **

$$

mpg = year*1.2300 

$$

```{r}
summary(time_vs_mpg <- lm(mpg ~ year, data = Auto))
```


b) Add `horsepower` on top of the variable `year` to your linear model. Is `year` still a significant variable at the .05 level? Give a precise interpretation of the `year`'s effect found here. 

> **Yes. Year is still a significant variable at the .05 level, which also means that when controlling for a cars horsepower, the year the car was made is still a relevant predictor of car mpg. From the output, we find that the beta estimate is 0.65727. This means that for every increase in unit of years, there is an increase of mpg by .65727 (Equation found below). This estimate is lower than what was found in the previous model**

$$

mpg = year*0.65727 + horsepower*(-0.13165)

$$

```{r}
summary(horsepower_time_vs_mpg <- lm(mpg ~ year + horsepower, data = Auto))
```


c) The two 95% CI's for the coefficient of year differ among (i) and (ii). How would you explain the difference to a non-statistician?

> **The 95% confidence interval represents that there is a 95% probability that the true linear regression line of a given population lies within this interval. As shown below, we see that the variable year has a different confidence interval across model 1 and model 2. Year in model 2 is much narrower than in model 1. This change is mostly dependent on how strongly correlated the additional variables are with a given outcome variable. Given the narrowing of confidence interval, this signifies that horsepower provides more information about our outcome variable mpg, which makes our model a better fit. **

```{r}
confint(time_vs_mpg)
confint(horsepower_time_vs_mpg)
```


d) Create a model with interaction by fitting `lm(mpg ~ year * horsepower)`. Is the interaction effect significant at .05 level? Explain the year effect (if any). 

> **An interaction effect offurs when a the effect of one variable on an outcome variable depends on the level of another variable. In this case, we find that year and horsepower have a significant interaction effect, indicating that the effect of year on mpg is stronger for vehicles with less horsepower. This is most likely due to horsepower having higher fuel consumption and thus negatively effects mpg. When looking at year, the later models of cars have better fuel efficiency. Therefore, cars that are newer and have less horsepower have higher mpg, whereas cars that are older and more horsepower have less mpg. **

```{r}
summary(horsepower_x_time_vs_mpg <- lm(mpg ~ year * horsepower, data =  Auto))

anova(horsepower_time_vs_mpg, horsepower_x_time_vs_mpg)
```


### Categorical predictors

Remember that the same variable can play different roles! Take a quick look at the variable `cylinders`, and try to use this variable in the following analyses wisely. We all agree that a larger number of cylinders will lower mpg. However, we can interpret `cylinders` as either a continuous (numeric) variable or a categorical variable.

a) Fit a model that treats `cylinders` as a continuous/numeric variable. Is `cylinders` significant at the 0.01 level? What effect does `cylinders` play in this model?

> **Given that larger number of cylinders lowers mpg, we expect to see a negative relationship between mpg and cylinders. When adding this to the model, this is exactly what we find. The p value is 5.0e-13, signifying this is a significant relationship and the coefficient is -1.5420. The coefficient indicates that when cylinder increases by a value of 1, mpg decreases by 1.5420. **

```{r}
Auto$cylinders <- as.numeric(Auto$cylinders)

Auto.continuous_cylinders <- Auto

summary(cylinders.cont_horsepower_x_time_vs_mpg <- lm(mpg ~ year*horsepower + cylinders, data = Auto.continuous_cylinders))
```


b) Fit a model that treats `cylinders` as a categorical/factor. Is `cylinders` significant at the .01 level? What is the effect of `cylinders` in this model? Describe the `cylinders` effect over `mpg`. 

> **By treating culinders as a factor, we find that the only significant effect is between vehicles that have 4 cylinders (p = 0.0044 **). Neither 5,6,8 are not significantly related to mpg. We also find an interesting effect with 4 cylinders and mpg where there is a positive relationship. This suggesting that vehicles with 4 cylinders have a positive effect on mpg. Given that higher cylinders means lower mpg, this makes sense considering that 4 cylinders is the smallest a vehicle in this dataset can have.**

```{r}
Auto$cylinders <- as.factor(Auto$cylinders)
Auto.categorical_cylinders <- Auto

summary(cylinders.fact_horsepower_x_time_vs_mpg <- lm(mpg ~ year*horsepower + cylinders, data = Auto.categorical_cylinders))
```


c) What are the fundamental differences between treating `cylinders` as a continuous and categorical variable in your models? 

> **Making assumptions on variables being continuous and categorical changes how we're able to interpret our results. By treating variables as a continuous variable, we assume that the variable has a linear relationship with an outcome variable and that this relationship is consistent across all levels of a variable. This part can be indicated when there was a negative relationship between cylinders and mpg, where the vehicles with less cylinders have higher mpg. By treating variables as a categorical variable, we assume that the relationship between the variable and an outcome variable is not linear and that it can take different forms at each level of the given variable. This can be seen when we only found a relationship between cylinder 4 and mpg. We are assuming that each cylinder group takes on an individual relationship with mpg. However, as can be seen by b, we didn't find category specific effects. **

d) Can you test the null hypothesis: fit0: `mpg` is linear in `cylinders` vs. fit1: `mpg` relates to `cylinders` as a categorical variable at .01 level?  

> **We do reject the null hypothesis given that the p value is 1.8e-07. This suggests that adding the variable as a categorical variable is able to provide us with more information about the model than the simple linear model created as a continuous variable**

```{r}
anova(cylinders.cont_horsepower_x_time_vs_mpg,cylinders.fact_horsepower_x_time_vs_mpg)
car::Anova(cylinders.cont_horsepower_x_time_vs_mpg)
car::Anova(cylinders.fact_horsepower_x_time_vs_mpg)
```


### Results

Final modeling question: we want to explore the effects of each feature as best as possible. You may explore interactions, feature transformations, higher order terms, or other strategies within reason. The model(s) should be as parsimonious (simple) as possible unless the gain in accuracy is significant from your point of view.
  
a) Describe the final model. Include diagnostic plots with particular focus on the model residuals and diagnoses.

> **To explore the effects of each car feature on mpg, we decided to look at mpg's association with all variables. Before we start our analysis, however, let's clean up our data. First, we used the function ggpairs to look at each association individually (this is shown above). We noticed that horsepower looks exponential. So, we decided to apply some function transformations. We looked at log and taking the inverse of it. Log didn't help, but the inverse did. So we decided to make a new variable called horsepower_new. Next, we looked at a description of our variables to see if they are coded properly. From the previous problem, we decided to keep cylinders as a factor since cylinders isn't quite a continous variable (cylinders_new). Additionally, we decided to apply a factor on origin. Origin represents the location of origin of the car model. 1 means its from America. 2 means its from Europe. 3 means its from Asia. We then analyzed a model with all variables considered and plotted all relationships grouped by cylinders and car origin. Here's what we found: **

- Displacement is not associated with mpg when considering all other variables

- Graphically, the number of cylinders affects mpg along with the relationship with other variables

- Generally, asian cars seem to have better mpg


```{r}
plot(Auto$horsepower, Auto$mpg) # relationship looks non linear

plot(log(Auto$horsepower), Auto$mpg)

plot(1/Auto$horsepower, Auto$mpg)

Auto_new <- ISLR::Auto %>% mutate(cylinders_new = as.factor(cylinders), horsepower_new = 1/horsepower, origin_new = as.factor(origin))

Auto.data <-  Auto_new %>% select(-horsepower, - cylinders, -name, -origin)


Auto.model <- lm(mpg~., Auto.data)

summary(Auto.model)

Anova(Auto.model)


## Car Cylinders 
ggplot(data = Auto.data, (aes(x = weight , y = mpg, col = cylinders_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()

ggplot(data = Auto.data, (aes(x = acceleration, y = mpg, col = cylinders_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()

ggplot(data = Auto.data, (aes(x = year, y = mpg, col = cylinders_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()

ggplot(data = Auto.data, (aes(x = origin_new, y = mpg, col = cylinders_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()


ggplot(data = Auto.data, (aes(x = horsepower_new, y = mpg, col = cylinders_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()

## Car Origin

ggplot(data = Auto.data, (aes(x = weight , y = mpg, col = origin_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()

ggplot(data = Auto.data, (aes(x = acceleration, y = mpg, col = origin_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()

ggplot(data = Auto.data, (aes(x = year, y = mpg, col = origin_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()


ggplot(data = Auto.data, (aes(x = horsepower_new, y = mpg, col = origin_new))) +
    geom_point()+
    geom_smooth(method = "lm", se = F) +
                theme_bw()

```


b) Summarize the effects found.

> In accordance with our previous findings, we removed displacement in our final model. Below, are our findings. A note to make about our results, we also tried different iterations of the origin factor to see how car models differ from each other. Here's what we found:

- All cylinder factors is possitively correlated when controlling for all other variables. 

- Comparing all car origin models, we find that European and American cars are negatively associated with mpg while Japanese cars are positively correlated with mpg.

- Inversing the horsepower variable was significantly and positively associated with mpg

- Year was significantly and positively associated with mpg

- Weight was significantly and negatively associated with mpg

- Acceleration was significantly and negatively associated with mpg

- f-statistic is 248 on 10 and 381 DF,  p-value: <2e-16

- RSE is 2.88 on 381 degrees of freedom

- Multiple R-squared is 0.867,

```{r}
## Final Model
Auto.data2 <- Auto.data %>% select(-displacement)
summary(Auto.model.revised <- lm(mpg~., Auto.data2))

Auto.data3 <-ISLR::Auto %>% mutate(cylinders_new = as.factor(cylinders), horsepower_new = 1/horsepower, origin_new = factor(origin, c(2,1,3))) %>% select(-horsepower, - cylinders, -name, -origin, - displacement)

summary(Auto.model.revised2 <- lm(mpg~., Auto.data3))

Auto.data4 <- ISLR::Auto %>% mutate(cylinders_new = as.factor(cylinders), horsepower_new = 1/horsepower, origin_new = factor(origin, c(3,1,2))) %>% select(-horsepower, - cylinders, -name, -origin, - displacement)

summary(Auto.model.revised3 <- lm(mpg~., Auto.data4))

```





c) Predict the `mpg` of the following car: A red car built in the US in 1983 that is 180 inches long, has eight cylinders, displaces 350 cu. inches, weighs 4000 pounds, and has a horsepower of 260. Also give a 95% CI for your prediction.

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} \\ 
\hline \\[-1.8ex] 
mpg\_newcar\_lowerbound & 1 & 0.270 &  & 0.270 & 0.270 \\ 
mpg\_newcar & 1 & 22.900 &  & 22.900 & 22.900 \\ 
mpg\_newcar\_upperbound & 1 & 46.100 &  & 46.100 & 46.100 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

```{r}
newcar = Auto.data4[1, ]
newcar[1] = NA
newcar[2] = 4000
newcar[3] = NA
newcar[4] = 1983
newcar[5] = 8
newcar[5] = as.factor(newcar[5])
newcar[6] = 1/260
newcar[7] = 1
newcar[7] = as.factor(newcar[7])

Auto.model.revised3$coefficients

mpg_newcar = -0.00296 * 4000 + 0.70009 *83 +  5.90870 +  915.85491* (1/260) + -2.10256 + -30.74449 

confint(Auto.model.revised3)

mpg_newcar_lowerbound = -0.00401 * 4000 +  0.61344 *83 +  2.63393 +  700.44708* (1/260) + -3.01824 + -36.91531 

mpg_newcar_upperbound = -1.91e-03 * 4000 + 7.87e-01 *83 +  9.18e+00 +  1.13e+03* (1/260) + -3.01824 + -2.21e+01 

(newcar_stat = data.frame(mpg_newcar_lowerbound, mpg_newcar, mpg_newcar_upperbound))
```


# Case study 1:  `ISLR::Auto` data

This will be the last part of the Auto data from ISLR. The original data contains 408 observations about cars. It has some similarity as the Cars data that we use in our lectures. To get the data, first install the package `ISLR`. The data set `Auto` should be loaded automatically. We use this case to go through methods learned so far. 

Final modelling question: We want to explore the effects of each feature as best as possible. 

1) Preparing variables: 

a) You may explore the possibility of variable transformations. We normally do not suggest to transform $x$ for the purpose of interpretation. You may consider to transform $y$ to either correct the violation of the linear model assumptions or if you feel a transformation of $y$ makes more sense from some theory. In this case we suggest you to look into `GPM=1/MPG`. Compare residual plots of MPG or GPM as responses and see which one might yield a more satisfactory patterns. 

In addition, can you provide some background knowledge to support the notion: it makes more sense to model `GPM`?  

> **When it comes to measuring the efficiency of car fuel intake, mpg has been a common metric for measuring this. However, gpm may be a better approach to take given the statisical tools currently under our belt. As shown by the plot below, gpm has a linear relationship amongst the variables, unlike mpg which has a non-linear relationship. Since we are using linear regressions to estimate a cars fuel efficiency, it makes sense to use a variable that is linear. Therefore, for the rest of our analysis, we will be focusing on gpm**

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} \\ 
\hline \\[-1.8ex] 
mpg & 392 & 23.400 & 7.800 & 9.000 & 46.600 \\ 
cylinders & 392 & 5.470 & 1.710 & 3 & 8 \\ 
displacement & 392 & 194.000 & 105.000 & 68.000 & 455.000 \\ 
horsepower & 392 & 104.000 & 38.500 & 46 & 230 \\ 
weight & 392 & 2,978.000 & 849.000 & 1,613 & 5,140 \\ 
acceleration & 392 & 15.500 & 2.760 & 8.000 & 24.800 \\ 
year & 392 & 76.000 & 3.680 & 70 & 82 \\ 
origin & 392 & 1.580 & 0.806 & 1 & 3 \\ 
gpm & 392 & 0.048 & 0.017 & 0.021 & 0.111 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 


\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & mpg & gpm \\ 
\\[-1.8ex] & (1) & (2)\\ 
\hline \\[-1.8ex] 
 cylinders & $-$0.493 & 0.002$^{***}$ \\ 
  & (0.323) & (0.001) \\ 
  & & \\ 
 displacement & 0.020$^{***}$ & $-$0.00003$^{**}$ \\ 
  & (0.008) & (0.00001) \\ 
  & & \\ 
 horsepower & $-$0.017 & 0.0001$^{***}$ \\ 
  & (0.014) & (0.00002) \\ 
  & & \\ 
 weight & $-$0.006$^{***}$ & 0.00001$^{***}$ \\ 
  & (0.001) & (0.00000) \\ 
  & & \\ 
 acceleration & 0.081 & 0.0003$^{**}$ \\ 
  & (0.099) & (0.0002) \\ 
  & & \\ 
 year & 0.751$^{***}$ & $-$0.001$^{***}$ \\ 
  & (0.051) & (0.0001) \\ 
  & & \\ 
 origin & 1.430$^{***}$ & $-$0.001$^{**}$ \\ 
  & (0.278) & (0.0005) \\ 
  & & \\ 
 Constant & $-$17.200$^{***}$ & 0.091$^{***}$ \\ 
  & (4.640) & (0.008) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & 392 & 392 \\ 
R$^{2}$ & 0.821 & 0.885 \\ 
Adjusted R$^{2}$ & 0.818 & 0.883 \\ 
Residual Std. Error (df = 384) & 3.330 & 0.006 \\ 
F Statistic (df = 7; 384) & 252.000$^{***}$ & 423.000$^{***}$ \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

```{r}
Auto <- ISLR::Auto %>% mutate(gpm = 1/mpg) %>% select(-name)

Auto.mpg <- lm(mpg ~ ., data = Auto[,1:8])

Auto.gpm <- lm(gpm ~ ., data = Auto[,2:9])

GGally::ggpairs(Auto)


car::Anova(Auto.mpg)

car::Anova(Auto.gpm)
```


b) You may also explore by adding interactions and higher order terms. The model(s) should be as *parsimonious* (simple) as possible, unless the gain in accuracy is significant from your point of view. 

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{gpm} \\ 
\\[-1.8ex] & (1) & (2)\\ 
\hline \\[-1.8ex] 
 cylinders & 0.002$^{***}$ & 0.001 \\ 
  & (0.001) & (0.001) \\ 
  & & \\ 
 displacement & $-$0.00003$^{**}$ & 0.00001 \\ 
  & (0.00001) & (0.00001) \\ 
  & & \\ 
 horsepower & 0.0001$^{***}$ & $-$0.0001$^{*}$ \\ 
  & (0.00002) & (0.00004) \\ 
  & & \\ 
 weight & 0.00001$^{***}$ & 0.00001$^{***}$ \\ 
  & (0.00000) & (0.00000) \\ 
  & & \\ 
 acceleration & 0.0003$^{**}$ & $-$0.001$^{***}$ \\ 
  & (0.0002) & (0.0003) \\ 
  & & \\ 
 year & $-$0.001$^{***}$ & $-$0.002$^{***}$ \\ 
  & (0.0001) & (0.0002) \\ 
  & & \\ 
 origin & $-$0.001$^{**}$ & $-$0.021$^{***}$ \\ 
  & (0.0005) & (0.008) \\ 
  & & \\ 
 acceleration:horsepower &  & 0.00002$^{***}$ \\ 
  &  & (0.00000) \\ 
  & & \\ 
 year:origin &  & 0.0003$^{***}$ \\ 
  &  & (0.0001) \\ 
  & & \\ 
 Constant & 0.091$^{***}$ & 0.147$^{***}$ \\ 
  & (0.008) & (0.016) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & 392 & 392 \\ 
R$^{2}$ & 0.885 & 0.895 \\ 
Adjusted R$^{2}$ & 0.883 & 0.892 \\ 
Residual Std. Error & 0.006 (df = 384) & 0.005 (df = 382) \\ 
F Statistic & 423.000$^{***}$ (df = 7; 384) & 361.000$^{***}$ (df = 9; 382) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

```{r}
## Creating a Model where there's an interaction term between acceleration and horsepower

summary(Auto.interaction_model.01 <-lm(gpm ~ cylinders + displacement + acceleration*horsepower + weight + year + origin, data = Auto[,2:9]))

## Our model appears to show sigificant difference between our models and that there may be an interaction between these two terms
anova(Auto.gpm,Auto.interaction_model.01)

## Let's try improving the model and adding more interactions weight and acceleration are not related
#summary(Auto.interaction_model.02 <-lm(gpm ~ cylinders + displacement + acceleration*horsepower + weight*acceleration + year + origin, data = Auto[,2:9]))

## Let's try improving the model and an interaction between year and origin
summary(Auto.interaction_model.03 <-lm(gpm ~ cylinders + displacement + acceleration*horsepower + weight + year*origin, data = Auto[,2:9]))

anova(Auto.interaction_model.03,Auto.interaction_model.01)

## Let's try removing some variables that aren't significant
summary(Auto.interaction_model.04 <-lm(gpm ~ acceleration*horsepower + weight + year*origin, data = Auto[,2:9]))

anova(Auto.interaction_model.04,Auto.interaction_model.03)

#stargazer(Auto.gpm,Auto.interaction_model.03)

```


c) Use Mallow's $C_p$ or BIC to select the model.

```{r}
Auto.fit.with.interaction <- regsubsets(gpm ~ cylinders + displacement + acceleration*horsepower + weight + year*origin, Auto[,2:9] , nvmax = 25, method="exhaustive")
A.I.<-summary(Auto.fit.with.interaction)

Auto.fit.with.no.interaction <- regsubsets(gpm ~., Auto[,2:9] , nvmax = 25, method="exhaustive")
A.NI <- summary(Auto.fit.with.no.interaction)


plot(A.I.$cp, xlab="Number of predictors",
ylab="Cp", main = "Auto with Interaction", col="red", pch=16, cex =3)

plot(A.NI$cp, xlab="Number of predictors",
ylab="Cp", main = "Auto without Interaction",col="red", pch=16, cex =3)


par(mfrow=c(1, 3)) # see diff criteria
plot(A.I.$cp, xlab="Number of predictors",
ylab="Cp", col="red", pch=16, cex=2)
plot(A.I.$rsq, xlab="Rˆ2", pch=15, col= "blue", cex=2)
plot(A.I.$rss, xlab="RSS", pch = 14, col="green", cex=2)
par(mfrow=c(1,1))

par(mfrow=c(1, 3)) # see diff criteria
plot(A.NI$cp, xlab="Number of predictors",
ylab="Cp", col="red", pch=16, cex=2)
plot(A.NI$rsq, xlab="Rˆ2", pch=15, col= "blue", cex=2)
plot(A.NI$rss, xlab="RSS", pch = 14, col="green", cex=2)
par(mfrow=c(1,1))


coef(Auto.fit.with.interaction,9)

coef(Auto.fit.with.no.interaction,7)

opt.size_intercation <- which.min(A.I.$cp)
opt.size_no_intercation <- which.min(A.NI$cp)

A.I.var <- A.I.$which # logic indicators which variables are in
A.I.var[opt.size_intercation,]

A.NI.var <- A.NI$which # logic indicators which variables are in
A.NI.var[opt.size_no_intercation,]

colnames(A.I.var)[A.I.var[opt.size_intercation,]]
colnames(A.NI.var)[A.NI.var[opt.size_no_intercation,]]

```


2) Describe the final model and its accuracy. Include diagnostic plots with particular focus on the model residuals.

```{r}
summary(Auto.final <-lm(gpm ~ cylinders + acceleration*horsepower + weight + year*origin, data = Auto[,2:9]))
par(mfrow=c(1,2))
plot(Auto.final, 1, cex =3) # max(fit.final$res) = 3;
plot(Auto.final, 2)
```


  * Summarize the effects found.
  
  > **In Part 1.B We explored different interaction terms and observed that our third model appears to show that there are interaction effects between origin and year & acceleration and horsepower. Following this, we attempted to reduce the model since there are a total of 9 variables. To reduce our model, we applied Mallow's $C_p$ to help us select the best model. When taking our model with interaction effects, we found that 8 variables is appropriate for our model. More specifically, we found that the variable displacement can be removed from our final model. Thus, we are using every variable except displacement in our model. Afterwards, we then applied diagnostic plots. Linearity/Homoscedasticity seems to be fine here; however, normality may be a concern**
  
  * Predict the `mpg` of a car that is: built in 1983, in the US, red, 180 inches long, 8 cylinders, 350 displacement, 260 as horsepower, and weighs 4,000 pounds. Give a 95% CI.
  

  \begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}} ccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
lower & mpg & upper \\ 
\hline \\[-1.8ex] 
$25.400$ & $37.000$ & $68.100$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 



```{r}
newcar = Auto[1, ]
newcar[1] = NA
newcar[2] = 8
newcar[3] = 350
newcar[4] = 260
newcar[5] = 4000
newcar[6] = 0
newcar[7] = 83
newcar[8] = 1
newcar[9] = NA
newcar[9] <-predict(Auto.final,newcar)
newcar[1] <- 1/newcar[9] 

newcar

newcar_confidence_intervals_gpm <- predict(Auto.final,newcar, interval="confidence", se.fit=TRUE)

newcar_confidence_intervals_mpg <- predict(Auto.final,newcar, interval="confidence", se.fit=TRUE)

newcar_confidence_intervals_mpg$fit <- 1/newcar_confidence_intervals_gpm$fit # Upper and Lower Need to be swapped

mpg <- newcar_confidence_intervals_mpg$fit[1]
upper <- newcar_confidence_intervals_mpg$fit[2]
lower <- newcar_confidence_intervals_mpg$fit[3]

newcar_mpg <- cbind(lower,mpg,upper)

```
  
  
  * Any suggestions as to how to improve the quality of the study?
  
  > **This study offers a great introduction into multiple linear regressions by showing how varying aspects of a car influence the mileage per gallon. However, there are limitations to this study that could be improved for the future. To start, these cars are from the 1970 - 1982. Cars have changed drastically since then, such as the existance of smart vehicles or new technical improvements (for example, average car horsepower in America is currently 200-250, whereas in this dataset it is 105). Therefore, having a new round of data collection can help us better entangle whether these variables are still important for modern cars. Another addition for this study, could be keeping track of whether a car is an electric car or not. Electric cars have better acceleration and mileage. If we make this modification, then we would also need to collect MPGe which is the mpg equivalent to electric cars. Another change we can make for this study is a comparison of different car companies. There is a list of vehicle names listed under the name variable, however, it would be helpful to just have a variable of what company this car is from. From our analysis, we found that Asian cars were more efficient, but what if this is largely explained by a specific group of car companies that may specialize in more efficient cars. **


# Case study 2: COVID19

See a seperate file covid_case_study.Rmd for details. 

